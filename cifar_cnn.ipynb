{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_CLASSES = 4\n",
    "CLASSES = [3,4,5,7]\n",
    "IMG_ROWS, IMG_COLS = 32, 32\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 3)\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset function\n",
    "def read_CIFAR(classes):\n",
    "    '''\n",
    "        Read and preprocess the dataset. \n",
    "        Extract the given classes images from labels (0 to 9) and return a one hot encoded version\n",
    "    '''\n",
    "    # load data with keras function\n",
    "    (images_train, labels_train), (images_test, labels_test) = cifar10.load_data()\n",
    "    # Image category selection\n",
    "    images_train, labels_train = extract_classes(images_train, labels_train, classes)\n",
    "    images_test, labels_test = extract_classes(images_test, labels_test, classes)\n",
    "    # One hot encoding of labels\n",
    "    labels_train = keras.utils.to_categorical(labels_train)\n",
    "    labels_test = keras.utils.to_categorical(labels_test)\n",
    "\n",
    "    return images_train, labels_train, images_test, labels_test\n",
    "\n",
    "def extract_classes(images, labels, classes):\n",
    "    '''\n",
    "        Extract the given classes in images\n",
    "    '''\n",
    "    # extract the first class of images\n",
    "    indices = (labels == classes[0]).reshape(labels.size)\n",
    "    images_return = images[indices]\n",
    "    labels_return = np.zeros(images_return.shape[0])\n",
    "    \n",
    "    # extend the images_return and lables_return with remaining classes\n",
    "    for i in range(1,len(classes)):\n",
    "        indices = (labels == classes[i]).reshape(labels.size)\n",
    "        images_return = np.concatenate((images_return, images[indices]),axis=0)\n",
    "        labels_return = np.concatenate((labels_return, np.ones(images_return.shape[0] - labels_return.shape[0]) * i))\n",
    "    \n",
    "    return images_return, labels_return\n",
    "\n",
    "def use_checkpoints(is_used):\n",
    "    if is_used:\n",
    "        from time import localtime\n",
    "        dt = localtime()\n",
    "        dt = str(dt.tm_mon) + \"_\" + str(dt.tm_mday) + \"_\" + str(dt.tm_hour) + \"_\" + str(dt.tm_min)\n",
    "        filepath = \"checkpoints/cifar4/dt_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "        return ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "\n",
    "# first conv followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='tanh', input_shape=INPUT_SHAPE))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "# second conv followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "# flatten the network and use Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# model.load_weights(\"checkpoints/cifar10/dt_weights-improvement-04-0.44.hdf5\")\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images_train, labels_train, images_test, labels_test = read_CIFAR(CLASSES)\n",
    "\n",
    "# Train model\n",
    "model.fit(images_train,\n",
    "          labels_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          shuffle=True,\n",
    "          verbose=1,\n",
    "          validation_split=0.33,\n",
    "          callbacks=[use_checkpoints(False)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
